{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fd04f5-9789-4e39-bc06-317133dcd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Importuri\n",
    "# -------------------------------\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536a053a-8b80-4b61-8105-6b0a1220f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Setup NLTK\n",
    "# -------------------------------\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e98147b-d7b5-42d7-bef6-c3cb086502c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Preconfigurare: stop words & stemmer\n",
    "# -------------------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# -------------------------------\n",
    "# Funcție de preprocesare\n",
    "# -------------------------------\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    lower = text.lower()\n",
    "    nopunct = [c for c in lower if c not in string.punctuation] #1.\n",
    "    nopunct = ''.join(nopunct)\n",
    "    # Alternativ, poți folosi regex dacă vrei să păstrezi $ și !:\n",
    "    # nopunct = re.sub(r\"[^a-z\\s$!]\", \"\", lower)\n",
    "    tokens = word_tokenize(nopunct)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -------------------------------\n",
    "# Încărcare și preprocesare dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
    "df[\"message\"] = df[\"message\"].apply(preprocess_text)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006b26a9-b2e2-4d8b-a5d7-2deefb6aa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Split: train/test\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"message\"], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0b891b-bb88-4fda-b7c4-b69bb246d3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978    1\n",
       "3989    0\n",
       "3935    0\n",
       "4078    0\n",
       "4086    1\n",
       "       ..\n",
       "3772    0\n",
       "5191    0\n",
       "5226    0\n",
       "5390    0\n",
       "860     0\n",
       "Name: label, Length: 4457, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a15d7122-8ebf-4989-9fee-ac0f75e2d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   free  free prize  money  money now  prize  prize now  win free  win money\n",
      "0     0           0      1          1      0          0         0          1\n",
      "1     1           1      0          0      1          1         1          0\n",
      "       free  free prize    money  money now     prize  prize now  win free  \\\n",
      "0  0.000000    0.000000  0.57735    0.57735  0.000000   0.000000  0.000000   \n",
      "1  0.447214    0.447214  0.00000    0.00000  0.447214   0.447214  0.447214   \n",
      "\n",
      "   win money  \n",
      "0    0.57735  \n",
      "1    0.00000  \n"
     ]
    }
   ],
   "source": [
    "# Exemplu de text (mesaje) cu CountVectorizer si TFIDFTRANSFORMER\n",
    "df = pd.DataFrame({\n",
    "    'message': [\n",
    "        \"Win money now\",\n",
    "        \"Win a free prize now\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Crează CountVectorizer cu bigrame\n",
    "# un min-df mai mic adauga cuvinte care apar foarte rar\n",
    "# un max_df mai mare de 90% va permite cuvintelor care sunt mai frecvente în setul de date să rămână în vocabular.\n",
    "# ngram_range=(1, 2) face unigram si bigram. Cu (1,3) face si trigram\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Obține matricea de frecvențe\n",
    "X_counts = vectorizer.fit_transform(df[\"message\"])\n",
    "# Afișează matricea de frecvențe completă\n",
    "X_counts_array = X_counts.toarray()\n",
    "\n",
    "# Obține termenii (unigramele și bigramele)\n",
    "df_terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Creează un DataFrame pentru a vizualiza frecvențele pentru primele 5 mesaje\n",
    "df_subset = pd.DataFrame(X_counts_array, columns=df_terms)\n",
    "\n",
    "# Afișează DataFrame-ul cu frecvențele\n",
    "print(df_subset)\n",
    "\n",
    "# Aplică TfidfTransformer pentru a transforma matricea de frecvențe într-o matrice TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=df_terms)\n",
    "\n",
    "# Afișează DataFrame-ul cu scorurile TF-IDF\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4182f0b-1a76-4ccb-b3a8-ed2861acbf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__max_depth': None, 'classifier__n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Pipeline și GridSearch\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# Pipeline cu [TfidfVectorizer] ORI cu [CountVectorizer] ORI cu [CountVectorizer+ TFID]\n",
    "# -------------------------------\n",
    "#CountVectorizer: transformă textul în frecvențe brute (cât de des apare un cuvânt/bigram/etc.).\n",
    "#TfidfTransformer: transformă acele frecvențe în scoruri TF-IDF, care penalizează cuvintele comune și dau mai multă greutate celor rare și informative.\n",
    "\n",
    "#vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "pipeline = Pipeline([ #Ordinea este importantă: vectorizer ➜ tfidf ➜ classifier.\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    #(\"classifier\", MultinomialNB()),\n",
    "    #(\"classifier\", LogisticRegression(solver=\"saga\", max_iter=5000))  # liblinear e bun pentru small/medium datasets\n",
    "    #(\"classifier\", LinearSVC())\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "# ======================================================================================\n",
    "# param grid pt MultinomialNB\n",
    "#param_grid = {\"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]}\n",
    "# ======================================================================================\n",
    "# Param grid pt Logistic Regression\n",
    "#param_grid = {\n",
    "#    \"classifier__C\": [0.01, 0.1, 1, 10],  # C = inverse regularization strength (cu cât e mai mic, cu atât regularizează mai puternic)\n",
    "#    \"classifier__penalty\": [\"l1\", \"l2\"]\n",
    "#}\n",
    "# =======================================================================================\n",
    "# Parametri pentru SVM\n",
    "#param_grid = {\n",
    "#    \"classifier__C\": [0.1, 1, 10],\n",
    "#    \"vectorizer__min_df\": [1],\n",
    "#    \"vectorizer__max_df\": [0.9],\n",
    "#}\n",
    "# ========================================================================================\n",
    "# Parametri pt RandomForest (poți ajusta mai mulți dacă vrei)\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100, 150],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,#cross-validation Modelul este antrenat pe 4 din cele 5 părți și testat pe cea de-a 5-a parte (foldul de testare).\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "### fara test_train split aplic training direct pe coloana mesage\n",
    "#grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "### cu train test split\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best model parameters:\", grid_search.best_params_)\n",
    "\n",
    "# ERROR on LOGISTIC REGRESSION\n",
    "#!!!!! ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(!!!! (1000 este by default)\n",
    "# Metoda 1 -> creste nr de iteratii: LogisticRegression(solver=\"liblinear\", max_iter=5000)\n",
    "# Metoda 2 -> incearca alt solver: LogisticRegression(solver=\"saga\", max_iter=1000) ... saga e optimizat pentru dataset-uri mari și pentru L1/L2 penalty.\n",
    "# Metoda 3 -> Ajustează C (regularizarea): \"classifier__C\": [0.1, 1, 10, 50] \n",
    "# Metoda 4 -> De obicei, Logistic Regression funcționează mai bine dacă faci scalare. Adaugă un StandardScaler înainte de classifier:\n",
    "# pipeline = Pipeline([\n",
    "#    (\"vectorizer\", CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))),\n",
    "#    (\"scaler\", StandardScaler(with_mean=False)),  # Normalizează datele\n",
    "#    (\"classifier\", LogisticRegression(solver=\"liblinear\", max_iter=1000))\n",
    "#])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92affe64-626d-4327-bfa6-be6afd25cf9d",
   "metadata": {},
   "source": [
    "# Intelegere Grid-Search\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    (\"vvvv\", CountVectorizer()),\n",
    "    (\"cccc\", MultinomialNB())\n",
    "])\n",
    "# MultinomialNB \n",
    "– parametri principali care pot fi ajustati de Grid Search: MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "Detalii despre fiecare parametru:\n",
    "\n",
    "1️⃣ - alpha (default = 1.0)\n",
    "\n",
    "    Este smoothing-ul Laplace (sau Lidstone).\n",
    "    Evită împărțirea la 0 când un cuvânt nu apare în training.\n",
    "    Valori mai mici = mai puțin smoothing (ex: alpha=0.1).\n",
    "    Valori mai mari = model mai conservator.\n",
    "\n",
    "✅ Este cel mai important de reglat\n",
    "\n",
    "2️⃣ - fit_prior (default = True)\n",
    "    \n",
    "    Decide dacă să estimeze automat distribuția prior a claselor (spam vs. ham).\n",
    "    Dacă False, presupune că toate clasele au probabilitate egală, indiferent de câte exemple sunt.\n",
    "\n",
    "3️⃣ - class_prior (default = None)\n",
    "    \n",
    "    Dacă fit_prior=False, poți seta manual tu distribuția claselor.\n",
    "    Ex: [0.7, 0.3] dacă știi că ai 70% ham și 30% spam.\n",
    "    Nu e folosit des, dar poate fi util în contexte specifice.\n",
    "\n",
    "Dacă vrei să ajustezi parametrii din MultinomialNB, scrii: \n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    \"cccc__alpha\": [0.01, 0.1, 0.5, 1.0],\n",
    "    \"cccc__fit_prior\": [True, False]\n",
    "}\n",
    "\n",
    "Dacă vrei să reglezi CountVectorizer, scrii:\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    \"vvvv__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vvvv__max_df\": [0.75, 0.9, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "460d1b0b-0462-4273-b38a-faf1395a7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': None, 'classifier__n_estimators': 150}\n",
      "0.8249466328308589\n",
      "                                              params  mean_test_score  \\\n",
      "0  {'classifier__max_depth': None, 'classifier__n...         0.809376   \n",
      "1  {'classifier__max_depth': None, 'classifier__n...         0.824786   \n",
      "2  {'classifier__max_depth': None, 'classifier__n...         0.824947   \n",
      "3  {'classifier__max_depth': 10, 'classifier__n_e...         0.148095   \n",
      "4  {'classifier__max_depth': 10, 'classifier__n_e...         0.165022   \n",
      "5  {'classifier__max_depth': 10, 'classifier__n_e...         0.151088   \n",
      "6  {'classifier__max_depth': 20, 'classifier__n_e...         0.528262   \n",
      "7  {'classifier__max_depth': 20, 'classifier__n_e...         0.515433   \n",
      "8  {'classifier__max_depth': 20, 'classifier__n_e...         0.508137   \n",
      "\n",
      "   rank_test_score  \n",
      "0                3  \n",
      "1                2  \n",
      "2                1  \n",
      "3                9  \n",
      "4                7  \n",
      "5                8  \n",
      "6                4  \n",
      "7                5  \n",
      "8                6  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'transform_input', 'verbose', 'vectorizer', 'tfidf', 'classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__preprocessor', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__monotonic_cst', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_) # best F1 score\n",
    "# Scoruri pentru toate combinațiile:\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(cv_results[[\"params\", \"mean_test_score\", \"rank_test_score\"]])\n",
    "# Dacă vrei să vezi toți parametrii care pot fi tunati într-un Pipeline\n",
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef860020-a072-4bf1-b020-4b43b50cc707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation on Test Set ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not-Spam       0.97      1.00      0.98       966\n",
      "        Spam       1.00      0.79      0.88       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.89      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Evaluare pe setul de test daca folosesc train_test\n",
    "# -------------------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\n--- Evaluation on Test Set ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Not-Spam\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa644f8c-048a-4928-876b-9cfddb3f6225",
   "metadata": {},
   "source": [
    "# Intelegere metrics\n",
    "# Când îți pasă mai mult să detectezi toate Spamurile (adică vrei recall bun)\n",
    "Ai 100 de mesaje, din care: 20 sunt Spam, 80 sunt Not-Spam! Modelul tău clasifică: 18 ca fiind Spam si 82 ca Not-Spam.\n",
    "\n",
    "Dintre cele 18: 15 sunt corect clasificate ca Spam (True Positives ✅), 3 sunt greșit clasificate ca Spam, dar sunt de fapt Not-Spam (False Positives ❌). Și a ratat 5 mesaje Spam, spunând că sunt Not-Spam (False Negatives ❌)\n",
    "\n",
    "Termen          |      Ce e                      | Valoare\n",
    "\n",
    "True Positives  | Spam detectat corect           | 15\n",
    "\n",
    "False Positives | Not-Spam clasificat ca Spam    | 3\n",
    "\n",
    "False Negatives | Spam ratat (zice că nu e spam) | 5\n",
    "\n",
    "Calcule:\n",
    "\n",
    "    Precision = Ce procent din mesajele prezise ca Spam erau chiar Spam?\n",
    "\n",
    "#Precision = TP / (TP+FP) = 15 / (15+3) = 15 / 18 = 0.83\n",
    "\n",
    "    Recall = Ce procent din mesajele reale Spam au fost detectate?\n",
    "\n",
    "#Recall = TP / TP+FN = 15 / 15+5 = 15 / 20 = 0.75\n",
    "\n",
    "Precision\t--> „Din ce am zis că e spam, câte erau cu adevărat?”\tCând vrei puține alarme false (ex. emailuri bune marcate ca spam)\n",
    "\n",
    "Recall\t    --> „Din toate spamurile reale, câte am prins?”\tCând vrei să ratezi cât mai puține spamuri (ex. filtru agresiv)\n",
    "\n",
    "# F1-score (media armonică dintre precision și recall) \n",
    "\n",
    "Adică:\n",
    "F1 = 2 ⋅ (precision⋅recall) / (precision+recall\n",
    "\n",
    "F1 = 2⋅(0.83+0.75) / (0.83⋅0.75) ≈ 0.79\n",
    "\n",
    "Intuiție:\n",
    "\n",
    "    Dacă precision este mare, dar recall e mic → Modelul e „prea timid”. Nu face multe greșeli, dar ratează spamuri.\n",
    "\n",
    "    Dacă recall este mare, dar precision e mic → Modelul e „prea agresiv”. Prinde aproape toate spamurile, dar alertează greșit și emailuri bune.\n",
    "\n",
    "    F1-score echilibrează ambele. E util mai ales când ai date dezechilibrate (ex: mult mai multe Not-Spam decât Spam).\n",
    "\n",
    "# Accuracy (Procentul de predicții corecte din total)\n",
    "Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "514172f7-4b58-4701-9bdf-49a592e8d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.48\n",
      "Not-Spam Probability: 0.52\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.16\n",
      "Not-Spam Probability: 0.84\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.02\n",
      "Not-Spam Probability: 0.98\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.52\n",
      "Not-Spam Probability: 0.48\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Mesaje de test și predicții\n",
    "# -------------------------------\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]\n",
    "\n",
    "processed_messages = [preprocess_text(msg) for msg in new_messages]\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)\n",
    "\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)\n",
    "\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]\n",
    "    ham_probability = prediction_probabilities[i][0]\n",
    "\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6857085-a929-4ef2-a79e-d14324dee6fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'decision_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m X_new = best_model.named_steps[\u001b[33m\"\u001b[39m\u001b[33mvectorizer\u001b[39m\u001b[33m\"\u001b[39m].transform(processed_messages)\n\u001b[32m     13\u001b[39m predictions = best_model.named_steps[\u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m].predict(X_new)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m scores = \u001b[43mbest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclassifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecision_function\u001b[49m(X_new)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_messages):\n\u001b[32m     17\u001b[39m     prediction = \u001b[33m\"\u001b[39m\u001b[33mSpam\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions[i] == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNot-Spam\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'RandomForestClassifier' object has no attribute 'decision_function'"
     ]
    }
   ],
   "source": [
    "########## SPECIAL PENTRU SVM #################\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]\n",
    "\n",
    "processed_messages = [preprocess_text(msg) for msg in new_messages]\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)\n",
    "\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "scores = best_model.named_steps[\"classifier\"].decision_function(X_new)\n",
    "\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    confidence = scores[i]\n",
    "\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Confidence Score: {confidence:.2f}\")  # mai mare => mai sigur\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12d1bbb-fd1d-497c-be7c-375d3bb34f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Salvare model\n",
    "# -------------------------------\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f9235ec-59cd-4ae2-b885-25a75f3a76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = joblib.load(model_filename)\n",
    "# predictions = loaded_model.predict(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff064852-afa0-48f6-be8d-900576ad41b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
